---
title: "Data Science 2 HW 3"
author: "Amanda Howarth"
date: "4/12/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo = T, message = FALSE, results='hide', warning=FALSE}
library(caret)
library(glmnet)
library(dplyr)
library(MASS) 
library(e1071)
library(pROC) 
library(AppliedPredictiveModeling)
library(ISLR)
library(tidyverse)
```

# PART A: Produce graphical summaries of the Weekly data
```{r}
data("Weekly")

#Summary of Weekly data
summary(Weekly)

ggplot(Weekly, aes(x = Year, y = Volume, group = Year)) + geom_boxplot() + labs(title = "Total Volume Across Years, 1990 - 2010",
           x = "Year",
           y = "Volume")
```
In the plot above, we can see that the total volume increases across years and the highest median volume was in 2009. 

# DENSITY PLOTS FOR PREDICTORS
```{r}
theme1 <- transparentTheme(trans = .4) 
theme1$strip.background$col <- rgb(.0, .6, .2, .2) 
trellis.par.set(theme1)

featurePlot(x = Weekly[, 1:7], 
            y = Weekly$Direction,
            scales = list(x=list(relation="free"), 
                          y=list(relation="free")),
            plot = "density", pch = "|", 
            auto.key = list(columns = 2))
```
Density plots are graphically presented above for seven predictor variables in the dataset (lag1, lag2, lag3, lag4, lag5, volume, year).


# PART B: Use full data set to perform a logistic regression with Direction as the response and the five LAg variables plus Volume as predictors. 
```{r}
glm.fit <- glm(Direction~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, 
               data = Weekly,
               family = binomial)

contrasts(Weekly$Direction)
#UP = 1; DOWN = 0

summary(glm.fit)
```
At alpha level of 0.05, only the variable "lag2" appears to be statistically significant in the logistic regression model (p-value = 0.0296). All other predictor variables have p-values greater than 0.05. 

# PART C: Compute the confusion matrix and overall fraction of correct predictions. Breifly explain what the confusion matrix is telling you. 
```{r}
glm.probs=predict(glm.fit, type = "response")
glm.probs[1:10]
contrasts(Weekly$Direction)
glm.pred=rep("Down", 1089)
glm.pred[glm.probs>.5]="Up"

#creating confusion matrix table
table(glm.pred, Weekly$Direction)
(557+54)/1089
# 0.5610652

mean(glm.pred == Weekly$Direction)

#using confusion matrix function
confusionMatrix(data = as.factor(glm.pred),
                reference = Weekly$Direction,
                positive = "Up")
```
The confusion matrix can be used to determine how many observations were correctly or incorrectly classified. The diagonal elments of the confusion matrix indicate correct predictions. From the output above, the accurcacy (number of cells with correct classification) was 56.11%. This value was calculated in the following way: (557 + 54)/1089 = 0.5611. 

The PPV value is 0.5643. Thus, of the total number of market UP predictions, only 56.43% were truly "UP". The NPV value is 0.5294. Of the total number of market DOWN predictions, only 52.94% of of those were truly "DOWN."

# Part D: Plot the ROC curve using the predicted probability from logistic regresion and report the AUC. 
```{r}
roc.glm <- roc(Weekly$Direction, glm.probs)

plot(roc.glm, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.glm), col = 4, add = TRUE) 
```
The AUC value is 0.554.

# PART E: Now fit the logistic regression model using a training data period from 1990 to 2008, with Lag1 and Lag2 as the predictors. Plot the ROC curve using the held out data (that is, the data from 2009 and 2010) and report the AUC.

```{r}
train = (Weekly$Year < 2009)
Weekly.2009=Weekly[!train,]
dim(Weekly.2009)

Direction.2009= Weekly$Direction[!train]
```
There are 104 observations in the test dataset from years 2009 and 2010. 

# LOGISTIC REG
```{r}
glm.fit2 <- glm(Direction~ Lag1 + Lag2, 
               data = Weekly, 
               subset = train, 
               family = binomial)


glm.probs2 =predict(glm.fit2, Weekly.2009, type="response")

glm.pred2 = rep("Down", 104)
glm.pred2[glm.probs2>.5]="Up"
table(glm.pred2, Direction.2009)
mean(glm.pred2 == Direction.2009)
mean(glm.pred2!=Direction.2009)

#using confusion matrix code
confusionMatrix(data = as.factor(glm.pred2),
                reference = Weekly.2009$Direction,
                positive = "Up")

roc.glm2 <- roc(Weekly.2009$Direction, glm.probs2)
plot(roc.glm2, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.glm2), col = 4, add = TRUE) 
```
The ROC curve using the held out data (that is, the data from 2009 and 2010) is presented above. The AUC value is 0.556.

# PART F: LDA
```{r}
lda.fit = lda(Direction ~ Lag1 + Lag2, data = Weekly, subset = train)

#Creates distributions (histograms) of Z for the two classes
plot(lda.fit)

lda.pred = predict(lda.fit, newdata = Weekly.2009)
head(lda.pred$posterior)
#posterior= predicted class probability

roc.lda <- roc(Weekly.2009$Direction, lda.pred$posterior[,2], 
               levels = c("Down", "Up"))

plot(roc.lda, legacy.axes = TRUE, print.auc = TRUE)
```
Using the held out data, the AUC value for the LDA model is 0.557. 

# PART F: QDA
```{r}
qda.fit <- qda(Direction ~ Lag1 + Lag2, 
               data = Weekly,
               subset = train)

qda.pred <- predict(qda.fit, newdata = Weekly.2009)
head(qda.pred$posterior)

roc.qda <- roc(Weekly.2009$Direction, qda.pred$posterior[,2], 
               levels = c("Down", "Up"))

plot(roc.qda, legacy.axes = TRUE, print.auc = TRUE)

```
The AUC value for the QDA model is 0.529. 

# PART G: KNN 
```{r}

ctrl <- trainControl(method = "repeatedcv",
                     repeats = 5,
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)

set.seed(1)
model.knn <- train(Direction ~ Lag1 + Lag2, 
                   data = Weekly, 
                   subset = train, 
                   method = "knn",
                   preProcess = c("center","scale"),
                   tuneGrid = data.frame(k = seq(1,200,by=5)),
                   trControl = ctrl)


#ROC using repreated cross-validation for Across Neighbors
ggplot(model.knn, print.auc = TRUE)
model.knn$bestTune
#k = 6 = optimal tuning paramter 

knnPredict <- predict(model.knn, newdata= Weekly.2009)
confusionMatrix(knnPredict, Weekly.2009$Direction)

#k=6
knnPredict2 <- predict(model.knn, newdata= Weekly.2009, type = "prob", k =6)
knn.roc <- roc(Weekly.2009$Direction, knnPredict2[,"Down"], levels = c("Down", "Up"))
plot(knn.roc, legacy.axes = TRUE, print.auc = TRUE)
``` 
The AUC value for the KNN model is 0.437. 

# RESULTS 
The following is a ranking of highest to lowest AUC values for the four models: LDA (0.557) > Logistic regressoin (0.556) > QDA (0.529) > KNN (0.437). The QDA and KNN models have lower AUC values. This is likley because they are more flexible models. Based on the AUC models, the LDA model would be the best. The ROC plots are plotted below for comparison. In the ROC plots, you can see that the LDA model typically has the highest values across values of 1-Specificity. 

# Evaluating test performance - ROC curves
```{r}
auc <- c(roc.glm2$auc[1], roc.lda$auc[1],
         roc.qda$auc[1], knn.roc$auc[1])

plot(roc.glm2, legacy.axes = TRUE)
plot(roc.glm, col = 2, add = TRUE)
plot(roc.lda, col = 3, add = TRUE)
plot(roc.qda, col = 4, add = TRUE)
plot(knn.roc, col = 5, add = TRUE)
modelNames <- c("glm model", "lda model","qda model", "knn model")
legend("bottomright", legend = paste0(modelNames, ": ", round(auc,3)),
       col = 1:6, lwd = 2)
```

